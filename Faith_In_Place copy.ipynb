{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MCDC Text Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Spacy packages for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and perform NLP with limited species set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a limited set of trees\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"en_core_web_sm\") # English model\n",
    "\n",
    "# Load your Excel data\n",
    "df = pd.read_excel(\"MCDC Sample Info-2.xlsx\")\n",
    "descriptions = df[\"Project Description\"].fillna(\"\")\n",
    "\n",
    "# USDA-based species list (expandable)\n",
    "usda_species = [\n",
    "    \"Red Maple\", \"Sugar Maple\", \"Silver Maple\", \"White Oak\", \"Bur Oak\", \"Northern Red Oak\",\n",
    "    \"Pin Oak\", \"Eastern Redbud\", \"Serviceberry\", \"Dogwood\", \"Black Walnut\", \"Bald Cypress\",\n",
    "    \"Ginkgo\", \"River Birch\", \"Honeylocust\", \"Eastern White Pine\", \"Loblolly Pine\",\n",
    "    \"Shortleaf Pine\", \"American Beech\", \"American Elm\", \"Black Cherry\", \"Tulip Tree\",\n",
    "    \"Hackberry\", \"Hickory\", \"Kentucky Coffeetree\", \"Sweetgum\", \"Sycamore\", \"Persimmon\",\n",
    "    \"Apple\", \"Peach\", \"Plum\", \"Pear\", \"Cherry\", \"Granny Smith\", \"Honeycrisp\", \"Jonathan\",\n",
    "    \"Fuji\", \"Stella\", \"Lapins\", \"Harvester\", \"Majestic\", \"Rome Beauty\"\n",
    "]\n",
    "\n",
    "# Setup spaCy matcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(name) for name in usda_species]\n",
    "matcher.add(\"USDA_TREE_SPECIES\", patterns)\n",
    "\n",
    "# Extraction function\n",
    "def extract_species(text):\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    return list(set(doc[start:end].text for _, start, end in matches))\n",
    "\n",
    "# Apply to your dataset\n",
    "df[\"USDA Matched Species\"] = descriptions.apply(extract_species)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"usda_species_extracted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and perform NLP with USDA species set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of species from USDA plant list\n",
    "# This function reads the USDA plant list file and extracts species names.\n",
    "df_plant_list = pd.read_csv(\"plant_checklist.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut Author out of scientific names if present\n",
    "def clean_scientific_name(name):\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    return \" \".join(name.split()[:2]) # Keep only the first two parts (genus and species)\n",
    "df_plant_list['clean_scientific_name'] = df_plant_list['Scientific Name with Author'].apply(clean_scientific_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all names into one list for matching\n",
    "name_set = pd.concat([\n",
    "    df_plant_list[\"clean_scientific_name\"].dropna(),\n",
    "    df_plant_list[\"Common Name\"].dropna()\n",
    "]).str.lower().unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"en_core_web_sm\") # English model\n",
    "\n",
    "# Load your Excel data\n",
    "df = pd.read_excel(\"MCDC Sample Info-2.xlsx\")\n",
    "descriptions = df[\"Project Description\"].fillna(\"\")\n",
    "\n",
    "# USDA-based species list (expandable)\n",
    "# Already defined in the previous section, but can be expanded as needed\n",
    "\n",
    "# Setup spaCy matcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(name) for name in name_set]\n",
    "matcher.add(\"USDA_TREE_SPECIES\", patterns)\n",
    "\n",
    "# Extraction function\n",
    "def extract_species(text):\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    return list(set(doc[start:end].text for _, start, end in matches))\n",
    "\n",
    "# Apply to your dataset\n",
    "df[\"USDA Matched Species\"] = descriptions.apply(extract_species)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"usda_species_extracted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Openai and Gemini API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gemini\n",
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction with Openai and Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyAPP-Ou6h2V_d8zSKjSLiJnDs1YJo7YyUo\")\n",
    "\n",
    "models = genai.list_models()\n",
    "for m in models:\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import google generative AI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set up the API key\n",
    "genai.configure(api_key= \"AIzaSyAPP-Ou6h2V_d8zSKjSLiJnDs1YJo7YyUo\")\n",
    "\n",
    "# Initialize the Gemini model\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# Send a prompt, engineering it until returning a Python dictionary\n",
    "response = model.generate_content(\"Extract the species and tree counts from this description. \"\n",
    "    \"Respond ONLY as a Python dictionary, e.g., {'Red Maple': 3, 'Honeycrisp apple': 4}, no extra text. \"\n",
    "    f\"Description: 'We will plant 3 Red Maple and 4 Honeycrisp apple trees.'\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the response to extract species and counts\n",
    "import re\n",
    "\n",
    "def parse_gemini_response(text):\n",
    "    result = []\n",
    "    # Split lines\n",
    "    lines = text.split('\\n')\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start extract species and tree counts from the descriptions from df\n",
    "# We need the result to be a list of dictionaries with species and counts\n",
    "def extract_species_and_counts(description):\n",
    "    # Send the description to Gemini\n",
    "    response = model.generate_content(\"Extract the species and tree counts from this description. \"\n",
    "    \"Respond ONLY as a Python dictionary, e.g., {'Red Maple': 3, 'Honeycrisp apple': 4}, no extra text. \"\n",
    "    f\"Description: '{description}'\")\n",
    "    \n",
    "    # Parse the response\n",
    "    text = response.text.strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_species_and_counts(\"Planting 3 Red Maple and 4 Honeycrisp apple trees.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will add the result to a new column in a csv file\n",
    "import pandas as pd\n",
    "# Load the existing CSV file\n",
    "df = pd.read_csv(\"usda_species_extracted.csv\")\n",
    "# Add a new column for the Gemini response\n",
    "df[\"Species from Gemini Response\"] = df[\"Project Description\"].apply(extract_species_and_counts)\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"usda_species_extracted_with_gemini.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Ollama Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ollama\n",
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"usda_species_extracted.csv\")\n",
    "\n",
    "# Initialize Ollama client\n",
    "model_name = 'mistral'  # You can also use 'llama3', 'gemma', etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Species Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"usda_species_extracted.csv\")\n",
    "\n",
    "# Initialize Ollama client\n",
    "model_name = 'mistral'  # You can also use 'llama3', 'gemma', etc.\n",
    "\n",
    "def extract_species_and_counts_ollama(description):\n",
    "    prompt = (\n",
    "        \"Extract the species and tree counts from this description. \"\n",
    "        \"Respond ONLY as a Python dictionary like {'Red Maple': 3, 'Honeycrisp apple': 4}. \"\n",
    "        \"The {'Red Maple': 3, 'Honeycrisp apple': 4} is just an example. We do not need to include Red Maple and Honeycrisp apple in the dictionary if it does not exist in descriptions.\"\n",
    "        \"If no species founded, return an empty dictionary {}. \"\n",
    "        \"If there are no specific counts for each kind of tree but there are total counts, divide the total counts equally among the species. \"\n",
    "        \"No explanation, no code, no extra text.\\n\"\n",
    "        f\"Description: {description}\"\n",
    "    )\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "\n",
    "    raw_text = response['message']['content'].strip()\n",
    "    print(\"RAW RESPONSE:\", raw_text)  # For debugging\n",
    "\n",
    "    # Safely parse to Python dictionary\n",
    "    try:\n",
    "        result = ast.literal_eval(raw_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {e}\")\n",
    "        result = {}\n",
    "\n",
    "    return result\n",
    "\n",
    "# Apply all rows\n",
    "df['Species from Ollama'] = df['Project Description'].apply(extract_species_and_counts_ollama)\n",
    "\n",
    "# Save result\n",
    "df.to_csv(\"usda_species_extracted_with_ollama.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Done! Saved to usda_species_extracted_with_ollama.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Goals Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will extract goals from the project goals\n",
    "import pandas as pd\n",
    "# Load the existing CSV file\n",
    "df_2 = pd.read_csv(\"usda_species_extracted_with_ollama.csv\")\n",
    "# Define a function to extract goals from the project description\n",
    "def extract_goals(project_goals):\n",
    "    prompt = (\n",
    "        \"Extract the goals from this project description. \"\n",
    "        \"Respond ONLY as a Python list of strings, e.g., ['Goal 1', 'Goal 2'], no extra text. \"\n",
    "        \"For each goal, conclude to make it precise and short.\"\n",
    "        f\"Description: '{project_goals}'\"\n",
    "    )\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    \n",
    "    raw_text = response['message']['content'].strip()\n",
    "    print(\"RAW RESPONSE:\", raw_text)  # For debugging\n",
    "    \n",
    "    try:\n",
    "        result = ast.literal_eval(raw_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {e}\")\n",
    "        result = []\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply the function to the 'Project Goals' column\n",
    "df_2['Goals from Ollama'] = df_2['Project Goals'].apply(extract_goals)\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_2.to_csv(\"usda_species_extracted_with_ollama_and_goals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Classification of Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will extract main goals/ percentage from the project goals\n",
    "import pandas as pd\n",
    "# Load the existing CSV file\n",
    "df_3 = pd.read_csv(\"usda_species_extracted_with_ollama_and_goals.csv\")\n",
    "# Define a function to extract goals from the project description\n",
    "def extract_goals_2(project_goals):\n",
    "    prompt = (\n",
    "        \"Extract the goals from this project description.\\n \"\n",
    "        \"After seeing the goals, conclude and classify them into main goals and their percentage.\\n \"\n",
    "        \"Respond ONLY as a Python dictionary, e.g., {Goal 1: 50%, Goal 2: 30%}, no extra text.\\n \"\n",
    "        \"The value should be a percentage calculated by (The total number of project goals associated with a certain classification) divided by the total project goals (The total length of the list)).\\n \"\n",
    "        \"The classifications are Environmental, Social, Economics, Urban Planning.\\n \"\n",
    "        \"With the logic given above, the total value of each list should add up to 100%. \\n\"\n",
    "        \"I want it to be in a precise pattern of {Environmental: 50%, Social: 30%, Economics: 10%, Urban Planning: 10%}, but these are just examples, you can change the values based on a principle I have given above. \\n\"\n",
    "        \"No explanation, no code, no extra text.\\n \"\n",
    "        \"No '' or any special characters in the response.\\n \"\n",
    "        f\"Description: '{project_goals}'\"\n",
    "    )\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    \n",
    "    raw_text = response['message']['content'].strip()\n",
    "    print(\"RAW RESPONSE:\", raw_text)  # For debugging\n",
    "    \n",
    "    try:\n",
    "        result = ast.literal_eval(raw_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {e}\")\n",
    "        result = []\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply the function to the 'Project Goals' column\n",
    "df_3['Main Goals'] = df_3['Goals from Ollama'].apply(extract_goals_2)\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_3.to_csv(\"usda_species_extracted_with_ollama_and_goals_2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data, clearing all things that are not spaces, commas, numbers, colon or letters\n",
    "import pandas as pd\n",
    "import re\n",
    "# Load the existing CSV file\n",
    "df_4 = pd.read_csv(\"usda_species_extracted_with_ollama_and_goals_2.csv\")\n",
    "# Remove all things that are not spaces, commas, colon, number or letters\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Keep only letters, numbers, spaces, commas, and colons\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s,:\\']', '', text)\n",
    "# Apply the cleaning function to the 'Main Goals' column\n",
    "df_4['Main Goals'] = df_4['Main Goals'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a new CSV file\n",
    "df_4.to_csv(\"usda_species_extracted_with_ollama_and_goals_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workforce Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import ollama  # Make sure you installed and running Ollama\n",
    "model_name = \"mistral\"  # Or whatever your active model is\n",
    "\n",
    "# Load data\n",
    "df_5 = pd.read_csv(\"usda_species_extracted_with_ollama_and_goals_3.csv\")\n",
    "\n",
    "# Define function to extract estimated workforce\n",
    "def extract_employment(trees_planted, workforce_development):\n",
    "    \n",
    "    if pd.isna(trees_planted) and pd.isna(workforce_development):\n",
    "        return None\n",
    "    prompt = (\n",
    "        \"Estimate the number of workers involved in the following tree planting project.\\n\"\n",
    "        \"Try to extract it directly from the Workforce Development description.\\n\"\n",
    "        \"If not available, try to estimate based on the number of trees planted.\\n\"\n",
    "        \"Return ONLY a number. If not possible, return None.\\n\\n\"\n",
    "        \"Please, no explanation, just a NUMBER.\\n\"\n",
    "        f\"Number of trees to be planted: {trees_planted}\\n\"\n",
    "        f\"Workforce Development description: {workforce_development}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        raw_text = response['message']['content'].strip()\n",
    "        print(\"RAW RESPONSE:\", raw_text)\n",
    "\n",
    "        # Try parsing as int or float, else return None\n",
    "        try:\n",
    "            result = int(raw_text)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                result = float(raw_text)\n",
    "            except ValueError:\n",
    "                result = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama error: {e}\")\n",
    "        result = None\n",
    "\n",
    "    return result\n",
    "\n",
    "# Apply to dataframe (this might take time if many rows)\n",
    "df_5[\"Estimated_Workforce\"] = df_5.apply(\n",
    "    lambda row: extract_employment(row.get(\"# Trees To Be Planted\", \"\"), row.get(\"Workforce Development\", \"\")),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the updated file\n",
    "df_5.to_csv(\"usda_species_with_estimated_workforce.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
